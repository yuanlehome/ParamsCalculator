import os
import re
import time
from typing import Any, Dict, Tuple

import pandas as pd
import plotly.express as px
import streamlit as st
from modelscope import AutoConfig, AutoModel

# --- é¡µé¢é…ç½® ---
st.set_page_config(page_title="ModelScope Model Params Calculator", page_icon="ğŸ§®", layout="wide")


# --- æ ¸å¿ƒé€»è¾‘å‡½æ•° ---
def format_number(num: int) -> str:
    """å°†æ•°å­—æ ¼å¼åŒ–ä¸º B (Billion) æˆ– M (Million)"""
    if num >= 1_000_000_000:
        return f"{num / 1_000_000_000:.2f} B"
    elif num >= 1_000_000:
        return f"{num / 1_000_000:.2f} M"
    else:
        return f"{num:,}"


def estimate_vram(param_count: int) -> Dict[str, str]:
    """ä¼°ç®—ä¸åŒç²¾åº¦ä¸‹çš„æƒé‡æ˜¾å­˜å ç”¨"""

    def bytes_to_gb(b):
        return f"{b / (1024 ** 3):.2f} GB"

    return {
        "FP32 (4 bytes)": bytes_to_gb(param_count * 4),
        "FP16/BF16 (2 bytes)": bytes_to_gb(param_count * 2),
        "Int8 (1 byte)": bytes_to_gb(param_count * 1),
        "Int4 (0.5 byte)": bytes_to_gb(param_count * 0.5),
    }


def estimate_kv_cache(model_config, context_length=2048, batch_size=1, dtype="fp16", tp=1):
    """ä¼°ç®— KV Cache æ˜¾å­˜å ç”¨"""
    num_layers = getattr(model_config, "num_hidden_layers", -1)
    num_heads = getattr(model_config, "num_key_value_heads", -1)
    hidden_size = getattr(model_config, "hidden_size", -1)
    head_dim = getattr(model_config, "head_dim", -1)

    if num_heads < 0 or hidden_size < 0:
        return "0.00 GB", {}
    if head_dim < 0:
        head_dim = hidden_size // num_heads

    dtype_size = {"fp32": 4, "fp16": 2, "bf16": 2, "fp8": 1, "int8": 1, "int4": 0.5}.get(dtype.lower(), 2)

    # è®¡ç®—å…¬å¼æ­¥éª¤
    calculation_steps = {
        "å±‚æ•°": num_layers,
        "æ³¨æ„åŠ›å¤´æ•°": num_heads,
        "æ¯ä¸ªå¤´çš„ç»´åº¦": head_dim,
        "ä¸Šä¸‹æ–‡é•¿åº¦": context_length,
        "æ‰¹å¤§å°": batch_size,
        "æ•°æ®ç±»å‹å­—èŠ‚æ•°": dtype_size,
        "KVå‘é‡æ•°": 2,  # Keyå’ŒValue
        "å¼ é‡å¹¶è¡Œåº¦": tp,
    }

    # è®¡ç®—å…¬å¼ï¼šlayers Ã— heads Ã— head_dim Ã— context_length Ã— 2 (K+V) Ã— batch_size Ã— dtype_size / tp
    kv_cache_bytes = num_layers * num_heads * head_dim * context_length * 2 * batch_size * dtype_size
    kv_cache_bytes /= tp

    return f"{kv_cache_bytes / (1024 ** 3):.2f} GB", calculation_steps


def extract_layer_index(name: str) -> int:
    """æå–å±‚ç¼–å·ç”¨äºæ’åº"""
    match = re.search(r"layers.(\d+)", name)
    return int(match.group(1)) if match else -1


def identify_param_type(name: str) -> str:
    """è¯†åˆ«å…³é”®å‚æ•°ç±»å‹"""
    name_lower = name.lower()
    if "embedding" in name_lower:
        return "embedding"
    elif any(k in name_lower for k in ["q_proj", "k_proj", "v_proj", "attn", "attention"]):
        return "attention"
    elif (
        "mlp" in name_lower
        or "fc" in name_lower
        or "gate" in name_lower
        or "up_proj" in name_lower
        or "down_proj" in name_lower
    ):
        return "mlp"
    elif "norm" in name_lower or "ln" in name_lower:
        return "norm"
    elif "lm_head" in name_lower or "head" in name_lower:
        return "head"
    else:
        return "other"


def get_dtype_size(dtype: str) -> float:
    """è·å–æ•°æ®ç±»å‹å¯¹åº”çš„å­—èŠ‚æ•°"""
    dtype_map = {"fp32": 4, "fp16": 2, "bf16": 2, "fp8": 1, "int8": 1, "int4": 0.5}
    return dtype_map.get(dtype.lower(), 2)


def calculate_model_params_detail(config):
    """è®¡ç®—æ¨¡å‹å‚æ•°é‡çš„è¯¦ç»†å…¬å¼ï¼Œæ”¯æŒDenseå’ŒMoEæ¨¡å‹"""
    details = {}
    formulas = []

    try:
        model_type = getattr(config, "model_type", "unknown").lower()
        vocab_size = getattr(config, "vocab_size", 0)
        hidden_size = getattr(config, "hidden_size", 0)
        num_layers = getattr(config, "num_hidden_layers", 0)
        num_attention_heads = getattr(config, "num_attention_heads", 0)
        intermediate_size = getattr(config, "intermediate_size", 0)

        # MoE ç›¸å…³å‚æ•°
        num_experts = getattr(config, "num_local_experts", getattr(config, "num_experts", 0))
        num_experts_per_tok = getattr(config, "num_experts_per_tok", getattr(config, "top_k", 0))
        expert_intermediate_size = getattr(
            config, "expert_intermediate_size", getattr(config, "ffn_hidden_size", intermediate_size)
        )

        # åˆ¤æ–­æ˜¯å¦æ˜¯MoEæ¨¡å‹
        is_moe_model = num_experts > 0
        if is_moe_model:
            model_type_with_moe = f"{model_type} (MoE)"
        else:
            model_type_with_moe = f"{model_type} (Dense)"

        # å­˜å‚¨åŸºç¡€ä¿¡æ¯
        details["åŸºç¡€ä¿¡æ¯"] = {
            "æ¨¡å‹ç±»å‹": model_type_with_moe,
            "è¯è¡¨å¤§å°": vocab_size,
            "éšè—å±‚ç»´åº¦": hidden_size,
            "å±‚æ•°": num_layers,
            "æ³¨æ„åŠ›å¤´æ•°": num_attention_heads,
            "ä¸­é—´å±‚ç»´åº¦": intermediate_size,
            "æ˜¯å¦MoE": "æ˜¯" if is_moe_model else "å¦",
        }

        if is_moe_model:
            details["åŸºç¡€ä¿¡æ¯"]["ä¸“å®¶æ•°é‡"] = num_experts
            details["åŸºç¡€ä¿¡æ¯"]["æ¯tokenä¸“å®¶æ•°"] = num_experts_per_tok
            details["åŸºç¡€ä¿¡æ¯"]["ä¸“å®¶ä¸­é—´å±‚ç»´åº¦"] = expert_intermediate_size

        # Transformerç±»æ¨¡å‹é€šç”¨è®¡ç®—å…¬å¼
        if (
            "llama" in model_type
            or "qwen" in model_type
            or "deepseek" in model_type
            or "ernie" in model_type
            or "mixtral" in model_type
        ):
            total_params = 0

            # 1. Embedding å‚æ•°
            embedding_params = vocab_size * hidden_size
            total_params += embedding_params
            formulas.append("### 1. Embedding å±‚å‚æ•°")
            formulas.append("è¯è¡¨å¤§å° Ã— éšè—å±‚ç»´åº¦")
            formulas.append(
                f"{format_number(vocab_size)} Ã— {format_number(hidden_size)} = **{format_number(embedding_params)}**"
            )

            # 2. Attention å‚æ•°ï¼ˆæ¯å±‚ï¼‰
            head_dim = hidden_size // num_attention_heads

            # QKVæŠ•å½±å‚æ•°
            qkv_params_per_layer = (hidden_size * hidden_size) * 3  # Q, K, V æŠ•å½±çŸ©é˜µ
            # OutputæŠ•å½±å‚æ•°
            output_proj_params = hidden_size * hidden_size

            attention_params_per_layer = qkv_params_per_layer + output_proj_params

            formulas.append("\n### 2. Attention å±‚å‚æ•°ï¼ˆæ¯å±‚ï¼‰")
            formulas.append("#### a) QKVæŠ•å½± (Q, K, V å„ä¸€ä¸ªçº¿æ€§å±‚)")
            formulas.append("éšè—å±‚ç»´åº¦ Ã— éšè—å±‚ç»´åº¦ Ã— 3")
            formulas.append(
                f"{format_number(hidden_size)} Ã— {format_number(hidden_size)} Ã— 3 = **{format_number(qkv_params_per_layer)}**"
            )

            formulas.append("\n#### b) OutputæŠ•å½±")
            formulas.append("éšè—å±‚ç»´åº¦ Ã— éšè—å±‚ç»´åº¦")
            formulas.append(
                f"{format_number(hidden_size)} Ã— {format_number(hidden_size)} = **{format_number(output_proj_params)}**"
            )

            formulas.append("\n#### c) æ¯å±‚Attentionæ€»å‚æ•°")
            formulas.append("QKVæŠ•å½± + OutputæŠ•å½±")
            formulas.append(
                f"{format_number(qkv_params_per_layer)} + {format_number(output_proj_params)} = **{format_number(attention_params_per_layer)}**"
            )

            # 3. MLP/FFN å‚æ•°ï¼ˆæ¯å±‚ï¼‰
            mlp_params_per_layer = 0
            mlp_calculation = []

            if intermediate_size > 0 or expert_intermediate_size > 0:
                if is_moe_model:
                    # MoE æ¨¡å‹è®¡ç®—
                    formulas.append("\n### 3. MoEå±‚å‚æ•°ï¼ˆæ¯å±‚ï¼‰")

                    # é—¨æ§ç½‘ç»œå‚æ•°ï¼ˆgateæˆ–routerï¼‰
                    gate_params = hidden_size * num_experts
                    mlp_params_per_layer += gate_params
                    mlp_calculation.append(
                        f"Gateç½‘ç»œ: {format_number(hidden_size)} Ã— {num_experts} = {format_number(gate_params)}"
                    )

                    # æ¯ä¸ªä¸“å®¶ï¼šgate_proj + up_proj + down_proj
                    expert_params_per_layer = (hidden_size * expert_intermediate_size) * 2 + (
                        expert_intermediate_size * hidden_size
                    )
                    mlp_calculation.append("æ¯ä¸ªä¸“å®¶å‚æ•°: gate_proj + up_proj + down_proj")
                    mlp_calculation.append(
                        f"  = ({format_number(hidden_size)} Ã— {format_number(expert_intermediate_size)} Ã— 2) + ({format_number(expert_intermediate_size)} Ã— {format_number(hidden_size)})"
                    )
                    mlp_calculation.append(f"  = {format_number(expert_params_per_layer)}")

                    # æ‰€æœ‰ä¸“å®¶æ€»å‚æ•°
                    all_experts_params = expert_params_per_layer * num_experts
                    mlp_params_per_layer += all_experts_params
                    mlp_calculation.append(
                        f"æ‰€æœ‰{num_experts}ä¸ªä¸“å®¶æ€»å‚æ•°: {format_number(expert_params_per_layer)} Ã— {num_experts} = {format_number(all_experts_params)}"
                    )

                    formulas.extend(mlp_calculation)

                    # MoEæ¿€æ´»å‚æ•°ï¼ˆæ¯ä¸ªtokenå®é™…æ¿€æ´»çš„å‚æ•°ï¼‰
                    active_params_per_layer = gate_params + (expert_params_per_layer * num_experts_per_tok)
                    formulas.append("\n#### d) æ¯tokenæ¿€æ´»çš„MoEå‚æ•°ï¼ˆæ¨ç†æ—¶ï¼‰")
                    formulas.append(f"Gateå‚æ•° + (æ¯ä¸ªä¸“å®¶å‚æ•° Ã— {num_experts_per_tok}ä¸ªæ¿€æ´»ä¸“å®¶)")
                    formulas.append(
                        f"{format_number(gate_params)} + ({format_number(expert_params_per_layer)} Ã— {num_experts_per_tok}) = **{format_number(active_params_per_layer)}**"
                    )

                    details["MoEä¿¡æ¯"] = {
                        "æ¯å±‚æ€»MoEå‚æ•°": mlp_params_per_layer,
                        "æ¯ä¸ªä¸“å®¶å‚æ•°": expert_params_per_layer,
                        "æ¯tokenæ¿€æ´»å‚æ•°": active_params_per_layer,
                        "ç¨€ç–æ€§": f"{(num_experts_per_tok / num_experts * 100):.1f}%",
                    }

                else:
                    # Denseæ¶æ„ï¼šgate_proj + up_proj + down_proj
                    mlp_params_per_layer = (hidden_size * intermediate_size) * 2 + (intermediate_size * hidden_size)

                    formulas.append("\n### 3. MLPå±‚å‚æ•°ï¼ˆæ¯å±‚ï¼‰- Llamaæ¶æ„")
                    formulas.append("#### a) gate_proj + up_proj")
                    formulas.append("éšè—å±‚ç»´åº¦ Ã— ä¸­é—´å±‚ç»´åº¦ Ã— 2")
                    formulas.append(
                        f"{format_number(hidden_size)} Ã— {format_number(intermediate_size)} Ã— 2 = **{format_number(hidden_size * intermediate_size * 2)}**"
                    )

                    formulas.append("\n#### b) down_proj")
                    formulas.append("ä¸­é—´å±‚ç»´åº¦ Ã— éšè—å±‚ç»´åº¦")
                    formulas.append(
                        f"{format_number(intermediate_size)} Ã— {format_number(hidden_size)} = **{format_number(intermediate_size * hidden_size)}**"
                    )

                    formulas.append("\n#### c) æ¯å±‚MLPæ€»å‚æ•°")
                    formulas.append(f"{format_number(mlp_params_per_layer)}")

            # 4. LayerNorm å‚æ•°ï¼ˆæ¯å±‚ï¼‰
            # ä¸¤ä¸ªLayerNormï¼šattentionä¹‹å‰çš„å’ŒMLPä¹‹å‰çš„
            # æ¯ä¸ªLayerNormï¼šgamma (hidden_size) + beta (hidden_size)
            norm_params_per_layer = hidden_size * 2 * 2  # 2ä¸ªLayerNormï¼Œæ¯ä¸ª2ä¸ªå‚æ•°

            formulas.append("\n### 4. LayerNorm å‚æ•°ï¼ˆæ¯å±‚ï¼‰")
            formulas.append("éšè—å±‚ç»´åº¦ Ã— 2ï¼ˆgammaå’Œbetaï¼‰Ã— 2ï¼ˆpre-attentionå’Œpre-MLPï¼‰")
            formulas.append(f"{format_number(hidden_size)} Ã— 2 Ã— 2 = **{format_number(norm_params_per_layer)}**")

            # 5. æ¯å±‚æ€»å‚æ•°
            if is_moe_model:
                # MoEæ¨¡å‹ï¼šæ¯å±‚å‚æ•° = Attention + MoE + LayerNorm
                params_per_layer = attention_params_per_layer + mlp_params_per_layer + norm_params_per_layer
                active_params_per_layer = (
                    attention_params_per_layer + details["MoEä¿¡æ¯"]["æ¯tokenæ¿€æ´»å‚æ•°"] + norm_params_per_layer
                )

                formulas.append("\n### 5. æ¯å±‚æ€»å‚æ•° (MoEæ¨¡å‹)")
                formulas.append("#### a) æ€»å‚æ•°ï¼ˆåŒ…å«æ‰€æœ‰ä¸“å®¶ï¼‰")
                formulas.append("Attention + MoE(æ‰€æœ‰ä¸“å®¶) + LayerNorm")
                formulas.append(
                    f"{format_number(attention_params_per_layer)} + {format_number(mlp_params_per_layer)} + {format_number(norm_params_per_layer)} = **{format_number(params_per_layer)}**"
                )

                formulas.append("\n#### b) æ¿€æ´»å‚æ•°ï¼ˆæ¯tokenå®é™…ä½¿ç”¨ï¼‰")
                formulas.append("Attention + MoE(æ¿€æ´»ä¸“å®¶) + LayerNorm")
                formulas.append(
                    f"{format_number(attention_params_per_layer)} + {format_number(details['MoEä¿¡æ¯']['æ¯tokenæ¿€æ´»å‚æ•°'])} + {format_number(norm_params_per_layer)} = **{format_number(active_params_per_layer)}**"
                )

                # å­˜å‚¨MoEæ¿€æ´»å‚æ•°
                details["MoEä¿¡æ¯"]["æ¯å±‚æ¿€æ´»å‚æ•°"] = active_params_per_layer
            else:
                # Denseæ¨¡å‹
                params_per_layer = attention_params_per_layer + mlp_params_per_layer + norm_params_per_layer

                formulas.append("\n### 5. æ¯å±‚æ€»å‚æ•°")
                formulas.append("Attention + MLP + LayerNorm")
                formulas.append(
                    f"{format_number(attention_params_per_layer)} + {format_number(mlp_params_per_layer)} + {format_number(norm_params_per_layer)} = **{format_number(params_per_layer)}**"
                )

            # 6. æ‰€æœ‰å±‚å‚æ•°
            all_layers_params = params_per_layer * num_layers
            total_params += all_layers_params

            if is_moe_model:
                # MoEæ¨¡å‹çš„æ¿€æ´»å‚æ•°æ€»é‡
                all_active_params = active_params_per_layer * num_layers
                formulas.append(f"\n### 6. æ‰€æœ‰{num_layers}å±‚æ€»å‚æ•° (MoEæ¨¡å‹)")
                formulas.append("#### a) æ€»å‚æ•°ï¼ˆåŒ…å«æ‰€æœ‰ä¸“å®¶ï¼‰")
                formulas.append("æ¯å±‚æ€»å‚æ•° Ã— å±‚æ•°")
                formulas.append(
                    f"{format_number(params_per_layer)} Ã— {num_layers} = **{format_number(all_layers_params)}**"
                )

                formulas.append("\n#### b) æ¿€æ´»å‚æ•°æ€»é‡ï¼ˆæ¯tokenå®é™…ä½¿ç”¨ï¼‰")
                formulas.append("æ¯å±‚æ¿€æ´»å‚æ•° Ã— å±‚æ•°")
                formulas.append(
                    f"{format_number(active_params_per_layer)} Ã— {num_layers} = **{format_number(all_active_params)}**"
                )

                details["MoEä¿¡æ¯"]["æ€»æ¿€æ´»å‚æ•°"] = all_active_params
            else:
                formulas.append(f"\n### 6. æ‰€æœ‰{num_layers}å±‚æ€»å‚æ•°")
                formulas.append("æ¯å±‚å‚æ•° Ã— å±‚æ•°")
                formulas.append(
                    f"{format_number(params_per_layer)} Ã— {num_layers} = **{format_number(all_layers_params)}**"
                )

            # 7. è¾“å‡ºå±‚ (LM Head) å‚æ•°
            lm_head_params = hidden_size * vocab_size
            total_params += lm_head_params

            formulas.append("\n### 7. è¾“å‡ºå±‚ (LM Head) å‚æ•°")
            formulas.append("éšè—å±‚ç»´åº¦ Ã— è¯è¡¨å¤§å°")
            formulas.append(
                f"{format_number(hidden_size)} Ã— {format_number(vocab_size)} = **{format_number(lm_head_params)}**"
            )

            # 8. æœ€ç»ˆæ€»è®¡
            if is_moe_model:
                formulas.append("\n### 8. æ¨¡å‹æ€»å‚æ•°é‡ (MoEæ¨¡å‹)")
                formulas.append("#### a) æ€»å‚æ•°ï¼ˆåŒ…å«æ‰€æœ‰ä¸“å®¶ï¼‰")
                formulas.append("Embedding + æ‰€æœ‰å±‚(æ€») + LM Head")
                formulas.append(
                    f"{format_number(embedding_params)} + {format_number(all_layers_params)} + {format_number(lm_head_params)} = **{format_number(total_params)}**"
                )

                # MoEæ¨¡å‹çš„æ¿€æ´»å‚æ•°æ€»é‡ï¼ˆæ¨ç†æ—¶ï¼‰
                total_active_params = embedding_params + all_active_params + lm_head_params
                formulas.append("\n#### b) æ¿€æ´»å‚æ•°æ€»é‡ï¼ˆæ¯tokenå®é™…ä½¿ç”¨ï¼‰")
                formulas.append("Embedding + æ‰€æœ‰å±‚(æ¿€æ´») + LM Head")
                formulas.append(
                    f"{format_number(embedding_params)} + {format_number(all_active_params)} + {format_number(lm_head_params)} = **{format_number(total_active_params)}**"
                )

                # è®¡ç®—ç¨€ç–ç‡å’Œæ¿€æ´»å‚æ•°æ¯”ä¾‹
                sparsity = (1 - (total_active_params / total_params)) * 100
                formulas.append("\n#### c) ç¨€ç–ç‡")
                formulas.append("1 - (æ¿€æ´»å‚æ•° / æ€»å‚æ•°)")
                formulas.append(
                    f"1 - ({format_number(total_active_params)} / {format_number(total_params)}) = **{sparsity:.1f}%**"
                )

                # å­˜å‚¨MoEç›¸å…³è®¡ç®—ç»“æœ
                details["è¯¦ç»†è®¡ç®—"] = {
                    "Embeddingå‚æ•°": embedding_params,
                    "æ¯å±‚Attentionå‚æ•°": attention_params_per_layer,
                    "æ¯å±‚MoEæ€»å‚æ•°": mlp_params_per_layer,
                    "æ¯å±‚æ¿€æ´»MoEå‚æ•°": details["MoEä¿¡æ¯"]["æ¯tokenæ¿€æ´»å‚æ•°"] if is_moe_model else 0,
                    "æ¯å±‚LayerNormå‚æ•°": norm_params_per_layer,
                    "æ¯å±‚æ€»å‚æ•°": params_per_layer,
                    "æ¯å±‚æ¿€æ´»å‚æ•°": active_params_per_layer if is_moe_model else params_per_layer,
                    "æ‰€æœ‰å±‚æ€»å‚æ•°": all_layers_params,
                    "æ‰€æœ‰å±‚æ¿€æ´»å‚æ•°": all_active_params if is_moe_model else all_layers_params,
                    "LM Headå‚æ•°": lm_head_params,
                    "æ€»è®¡ï¼ˆå«æ‰€æœ‰ä¸“å®¶ï¼‰": total_params,
                    "æ€»è®¡ï¼ˆæ¿€æ´»å‚æ•°ï¼‰": total_active_params if is_moe_model else total_params,
                    "ç¨€ç–ç‡": f"{sparsity:.1f}%",
                }
            else:
                formulas.append("\n### 8. æ¨¡å‹æ€»å‚æ•°é‡")
                formulas.append("Embedding + æ‰€æœ‰å±‚ + LM Head")
                formulas.append(
                    f"{format_number(embedding_params)} + {format_number(all_layers_params)} + {format_number(lm_head_params)} = **{format_number(total_params)}**"
                )

                # å­˜å‚¨è¯¦ç»†è®¡ç®—ç»“æœ
                details["è¯¦ç»†è®¡ç®—"] = {
                    "Embeddingå‚æ•°": embedding_params,
                    "æ¯å±‚Attentionå‚æ•°": attention_params_per_layer,
                    "æ¯å±‚MLPå‚æ•°": mlp_params_per_layer,
                    "æ¯å±‚LayerNormå‚æ•°": norm_params_per_layer,
                    "æ¯å±‚æ€»å‚æ•°": params_per_layer,
                    "æ‰€æœ‰å±‚æ€»å‚æ•°": all_layers_params,
                    "LM Headå‚æ•°": lm_head_params,
                    "æ€»è®¡": total_params,
                }

            details["å…¬å¼"] = formulas

        else:
            formulas.append(f"æ¨¡å‹ç±»å‹ '{model_type}' çš„è¯¦ç»†è®¡ç®—å…¬å¼æœªå®ç°")
            details["å…¬å¼"] = formulas

    except Exception as e:
        formulas.append(f"è®¡ç®—è¯¦ç»†å…¬å¼æ—¶å‡ºé”™: {str(e)}")
        details["å…¬å¼"] = formulas

    return details


@st.cache_data(show_spinner=False)
def analyze_model_structure(model_id: str, trust_remote_code: bool) -> Tuple[bool, Any, pd.DataFrame, str, Any]:
    """
    ä¸‹è½½ Config å¹¶å®ä¾‹åŒ– Meta Modelï¼Œç»Ÿè®¡å‚æ•°ã€‚
    è¿”å›: (æ˜¯å¦æˆåŠŸ, ç»Ÿè®¡ä¿¡æ¯å­—å…¸, è¯¦ç»†å‚æ•°DataFrame, é”™è¯¯ä¿¡æ¯, configå¯¹è±¡)
    """
    try:
        # è®¾ç½®ç¼“å­˜è·¯å¾„
        cache_dir = os.path.join(os.path.expanduser("~"), ".cache", "modelscope", "hub")
        os.makedirs(cache_dir, exist_ok=True)

        # ä»ModelScopeåŠ è½½é…ç½®
        try:
            config = AutoConfig.from_pretrained(model_id, trust_remote_code=trust_remote_code, cache_dir=cache_dir)
            st.success(f"âœ… æˆåŠŸä»ModelScopeåŠ è½½é…ç½®: {model_id}")
        except Exception as e:
            return False, None, None, f"ModelScopeé…ç½®åŠ è½½å¤±è´¥: {str(e)}", None

        # è®¡ç®—è¯¦ç»†å…¬å¼
        detail_calculation = calculate_model_params_detail(config)

        # å°è¯•åŠ è½½æ¨¡å‹ç»“æ„ï¼ˆä½¿ç”¨meta tensorï¼‰
        try:
            from accelerate import init_empty_weights

            with init_empty_weights():
                model = AutoModel.from_config(config, trust_remote_code=trust_remote_code)
        except ImportError:
            # å¦‚æœaccelerateä¸å¯ç”¨ï¼Œå°è¯•ç›´æ¥åŠ è½½ä½†æ•è·é”™è¯¯
            st.warning("âš ï¸ accelerateåº“æœªå®‰è£…ï¼Œä½¿ç”¨æ™®é€šæ–¹å¼åŠ è½½ï¼ˆå¯èƒ½æ¶ˆè€—æ›´å¤šå†…å­˜ï¼‰")
            try:
                model = AutoModel.from_config(config, trust_remote_code=trust_remote_code)
            except Exception as e:
                return False, None, None, f"æ¨¡å‹ç»“æ„åˆå§‹åŒ–å¤±è´¥ï¼ˆæ— meta tensorï¼‰: {str(e)}", None

        total_params = 0
        trainable_params = 0
        param_data = []

        for name, param in model.named_parameters():
            num_params = param.numel()
            total_params += num_params
            if param.requires_grad:
                trainable_params += num_params

            param_data.append(
                {
                    "Full Name": name,
                    "Group": name.split(".")[0] if len(name.split(".")) > 0 else "base",
                    "SubGroup": name.split(".")[1] if len(name.split(".")) > 1 else "other",
                    "Shape": str(tuple(param.shape)),
                    "Count": num_params,
                    "Dtype": str(param.dtype).replace("torch.", ""),
                    "LayerIdx": extract_layer_index(name),
                    "ParamType": identify_param_type(name),
                }
            )

        df_params = pd.DataFrame(param_data)

        # ä½¿ç”¨è¯¦ç»†è®¡ç®—ä¸­çš„æ€»è®¡ï¼Œæˆ–è€…ä½¿ç”¨ç»Ÿè®¡çš„æ€»è®¡
        calculated_total = detail_calculation.get("è¯¦ç»†è®¡ç®—", {}).get("æ€»è®¡", 0)
        if calculated_total > 0:
            final_total = calculated_total
        else:
            final_total = total_params

        info = {
            "model_type": getattr(config, "model_type", "unknown"),
            "total_params": final_total,
            "trainable_params": trainable_params,
            "architectures": getattr(config, "architectures", ["Unknown"]),
            "vocab_size": getattr(config, "vocab_size", "N/A"),
            "hidden_size": getattr(config, "hidden_size", "N/A"),
            "num_layers": getattr(config, "num_hidden_layers", 0),
            "num_heads": getattr(config, "num_attention_heads", 0),
            "max_position_embeddings": getattr(config, "max_position_embeddings", "N/A"),
            "intermediate_size": getattr(config, "intermediate_size", "N/A"),
            "source": "ModelScope",
            "detail_calculation": detail_calculation,
        }

        return True, info, df_params, "", config
    except Exception as e:
        return False, None, None, str(e), None


# --- UI å¸ƒå±€ ---
st.title("ğŸ§® ModelScope æ¨¡å‹å‚æ•°é€è§†é•œ")
st.markdown(
    """
æ­¤å·¥å…·é€šè¿‡è¯»å– ModelScope æ¨¡å‹çš„ `config.json` å¹¶æ„å»º **Meta Tensor** æ¥è®¡ç®—å‚æ•°é‡ã€‚
**ç‰¹ç‚¹ï¼š** æ— éœ€ä¸‹è½½åºå¤§æƒé‡æ–‡ä»¶ï¼Œç§’çº§åˆ†æ 70B+ æ¨¡å‹ï¼ŒèŠ‚çœå†…å­˜ï¼Œ**å¹¶å±•ç¤ºè¯¦ç»†çš„å‚æ•°é‡è®¡ç®—å…¬å¼**ã€‚
"""
)

with st.sidebar:
    st.header("è®¾ç½®")

    # æ¨¡å‹è¾“å…¥
    model_input = st.text_input(
        "ModelScope æ¨¡å‹ ID",
        value="Qwen/Qwen3-235B-A22B-Instruct-2507-FP8",
        help="æ ¼å¼ï¼šç»„ç»‡å/æ¨¡å‹åï¼Œå¦‚ Qwen/Qwen3-235B-A22B-Instruct-2507-FP8",
    )

    trust_remote = st.checkbox(
        "Trust Remote Code", value=True, help="å¤§å¤šæ•°ModelScopeæ¨¡å‹éœ€è¦æ­¤é€‰é¡¹ï¼Œå¦åˆ™å¯èƒ½æ— æ³•åŠ è½½é…ç½®"
    )

    st.divider()
    st.subheader("æ¨ç†é…ç½®")
    # ä¿®æ”¹ä¸Šä¸‹æ–‡é•¿åº¦é€‰æ‹©
    context_options = {
        "1K (1024)": 1024,
        "4K (4096)": 4096,
        "8K (8192)": 8192,
        "16K (16384)": 16384,
        "32K (32768)": 32768,
        "64K (65536)": 65536,
        "128K (131072)": 131072,
        "è‡ªå®šä¹‰": "custom",
    }

    context_choice = st.selectbox(
        "ä¸Šä¸‹æ–‡é•¿åº¦", options=list(context_options.keys()), index=4, help="é€‰æ‹©é¢„è®¾é•¿åº¦æˆ–è‡ªå®šä¹‰"  # é»˜è®¤é€‰æ‹© 32K
    )

    if context_choice == "è‡ªå®šä¹‰":
        context_length = st.number_input(
            "è¾“å…¥è‡ªå®šä¹‰ä¸Šä¸‹æ–‡é•¿åº¦",
            value=32768,
            min_value=1,
            max_value=1_000_000,
            step=1024,
            help="è¾“å…¥å…·ä½“çš„ä¸Šä¸‹æ–‡é•¿åº¦å€¼",
        )
    else:
        context_length = context_options[context_choice]
    batch_size = st.number_input("æ‰¹å¤§å°", value=8, min_value=1, step=1, help="æ¨ç†æ—¶çš„æ‰¹é‡å¤§å°")
    tp = st.number_input("å¼ é‡å¹¶è¡Œåº¦ (TP)", value=2, min_value=1, step=1, help="æ¨¡å‹å¹¶è¡Œåº¦ï¼Œé€šå¸¸ç”¨äºå¤šå¡æ¨ç†")
    dtype_select = st.selectbox(
        "KV Cache æ•°æ®ç±»å‹",
        options=["fp16", "bf16", "fp32", "fp8", "int8", "int4"],
        index=0,
        help="KV Cache å­˜å‚¨çš„æ•°æ®ç²¾åº¦",
    )

# å¦‚æœæœ‰session stateä¸­çš„æ¨¡å‹IDï¼Œæ›´æ–°è¾“å…¥æ¡†
if "model_input" in st.session_state:
    model_input = st.session_state.model_input

run_btn = st.button("ğŸš€ å¼€å§‹åˆ†æ", type="primary", width="stretch")

if run_btn and model_input:
    status_container = st.status("æ­£åœ¨è¿æ¥ ModelScope...", expanded=True)
    start_time = time.time()

    with status_container:
        st.write("ğŸ“¡ æ•°æ®æº: ModelScope")
        st.write(f"ğŸ” æ¨¡å‹ID: {model_input}")
        st.write("ğŸ“Š æ­£åœ¨è®¡ç®—è¯¦ç»†å‚æ•°å…¬å¼...")

    success, info, df, error_msg, config = analyze_model_structure(model_input, trust_remote)

    if success:
        elapsed_time = time.time() - start_time
        status_container.update(label=f"âœ… åˆ†æå®Œæˆï¼è€—æ—¶ {elapsed_time:.2f}ç§’", state="complete", expanded=False)

        # --- ä¸»æ˜¾ç¤ºåŒºåŸŸ ---
        tab_overview, tab_formula, tab_details, tab_viz = st.tabs(
            ["ğŸ“Š æ¦‚è§ˆ", "ğŸ§® è¯¦ç»†å…¬å¼", "ğŸ” è¯¦ç»†å‚æ•°", "ğŸ“ˆ å¯è§†åŒ–"]
        )

        with tab_overview:
            st.subheader("ğŸ“Š æ¨¡å‹åŸºæœ¬ä¿¡æ¯")

            col1, col2, col3, col4 = st.columns(4)
            col1.metric("æ€»å‚æ•°é‡", format_number(info["total_params"]))
            col2.metric("æ¨¡å‹æ¶æ„", info["model_type"])
            col3.metric("éšè—å±‚ç»´åº¦", info["hidden_size"])
            col4.metric("è¯è¡¨å¤§å°", info["vocab_size"])

            col5, col6, col7, col8 = st.columns(4)
            col5.metric("å±‚æ•°", info["num_layers"])
            col6.metric("æ³¨æ„åŠ›å¤´æ•°", info["num_heads"])
            col7.metric("ä¸­é—´å±‚å¤§å°", info["intermediate_size"])
            col8.metric("æœ€å¤§åºåˆ—é•¿åº¦", info["max_position_embeddings"])

            # æ˜¾ç¤ºMoEä¿¡æ¯ï¼ˆå¦‚æœé€‚ç”¨ï¼‰
            detail_info = info.get("detail_calculation", {})
            if "MoEä¿¡æ¯" in detail_info:
                st.subheader("ğŸ§© MoE æ¨¡å‹ä¿¡æ¯")
                moe_info = detail_info["MoEä¿¡æ¯"]

                col9, col10, col11, col12 = st.columns(4)
                if "ä¸“å®¶æ•°é‡" in moe_info:
                    col9.metric("ä¸“å®¶æ•°é‡", moe_info["ä¸“å®¶æ•°é‡"])
                if "æ¯tokenä¸“å®¶æ•°" in moe_info:
                    col10.metric("æ¯tokenä¸“å®¶æ•°", moe_info["æ¯tokenä¸“å®¶æ•°"])
                if "ç¨€ç–æ€§" in moe_info:
                    col11.metric("ç¨€ç–ç‡", moe_info["ç¨€ç–æ€§"])
                if "æ€»è®¡ï¼ˆæ¿€æ´»å‚æ•°ï¼‰" in detail_info["è¯¦ç»†è®¡ç®—"]:
                    col12.metric("æ¿€æ´»å‚æ•°", format_number(detail_info["è¯¦ç»†è®¡ç®—"]["æ€»è®¡ï¼ˆæ¿€æ´»å‚æ•°ï¼‰"]))

            # æ˜¾ç¤ºæ•°æ®æº
            st.info(f"ğŸ“¡ æ•°æ®æº: {info['source']}")

            # --- æƒé‡æ˜¾å­˜ ---
            st.subheader("ğŸ’¾ ç†è®ºæ˜¾å­˜å ç”¨ (ä»…æƒé‡)")
            vram_info = estimate_vram(info["total_params"])
            v_cols = st.columns(4)
            for idx, (dtype, size) in enumerate(vram_info.items()):
                v_cols[idx].info(f"**{dtype}**\n\n{size}")

            # --- KV Cache æ˜¾å­˜ ---
            if info["num_layers"] > 0 and info["num_heads"] > 0:
                kv_size, kv_steps = estimate_kv_cache(config, context_length, batch_size, dtype_select, tp)
                st.info(
                    f"âš¡ KV Cache æ˜¾å­˜ä¼°ç®— ({dtype_select}, context={context_length}, batch={batch_size}, TP={tp}): {kv_size}"
                )

                with st.expander("æŸ¥çœ‹KV Cacheè®¡ç®—å…¬å¼"):
                    st.write("### KV Cache è®¡ç®—å…¬å¼")
                    # åˆ›å»ºä¸¤åˆ—å¸ƒå±€ï¼Œè®©å…¬å¼æœ‰æ›´å¤šç©ºé—´
                    col_formula, col_explanation = st.columns([1, 1])

                    with col_formula:
                        st.markdown("**è®¡ç®—å…¬å¼:**")
                        # ä½¿ç”¨æ›´ç®€æ´çš„ LaTeX å…¬å¼å¹¶ç¡®ä¿æ­£ç¡®æ˜¾ç¤º
                        st.latex(r"""\text{KB} = \frac{L \times H \times D \times C \times 2 \times B \times S}{TP}""")

                    with col_explanation:
                        st.markdown("**å˜é‡è¯´æ˜:**")
                        st.markdown("- $L$ = å±‚æ•°")
                        st.markdown("- $H$ = æ³¨æ„åŠ›å¤´æ•°")
                        st.markdown("- $D$ = æ¯ä¸ªå¤´çš„ç»´åº¦")
                        st.markdown("- $C$ = ä¸Šä¸‹æ–‡é•¿åº¦")
                        st.markdown("- $B$ = æ‰¹å¤§å°")
                        st.markdown("- $S$ = æ•°æ®ç±»å‹å­—èŠ‚æ•°")
                        st.markdown("- $TP$ = å¼ é‡å¹¶è¡Œåº¦")
                        st.markdown("- $2$ = Key å’Œ Value ä¸¤ä¸ªå‘é‡")

                    st.write("**è®¡ç®—æ­¥éª¤:**")
                    for key, value in kv_steps.items():
                        st.write(f"- {key}: {value}")

                    st.write("\n**å…·ä½“è®¡ç®—:**")
                    st.write(
                        f"{info['num_layers']} Ã— {info['num_heads']} Ã— {info['hidden_size'] // info['num_heads']} Ã— {context_length} Ã— 2 Ã— {batch_size} Ã— {get_dtype_size(dtype_select)} Ã· {tp}"
                    )
                    st.write(f"= {kv_size}")
            else:
                st.warning("âš ï¸ æ— æ³•è®¡ç®—KV Cacheï¼šæ¨¡å‹å±‚æ•°æˆ–æ³¨æ„åŠ›å¤´æ•°ä¸º0")

        with tab_formula:
            st.subheader("ğŸ§® å‚æ•°è¯¦ç»†è®¡ç®—å…¬å¼")

            detail_info = info.get("detail_calculation", {})

            # æ˜¾ç¤ºåŸºç¡€ä¿¡æ¯
            if "åŸºç¡€ä¿¡æ¯" in detail_info:
                st.write("### æ¨¡å‹é…ç½®ä¿¡æ¯")
                base_info = detail_info["åŸºç¡€ä¿¡æ¯"]
                info_cols = st.columns(3)
                info_items = list(base_info.items())

                for i in range(0, len(info_items), 3):
                    for j in range(3):
                        if i + j < len(info_items):
                            key, value = info_items[i + j]
                            info_cols[j].metric(key, value)

            # æ˜¾ç¤ºè¯¦ç»†å…¬å¼
            if "å…¬å¼" in detail_info:
                st.write("### å‚æ•°é‡è®¡ç®—å…¬å¼æ¨å¯¼")

                # åˆ›å»ºä¸€ä¸ªå¯æŠ˜å çš„ä»£ç åŒºåŸŸæ˜¾ç¤ºå…¬å¼
                formula_text = "\n".join(detail_info["å…¬å¼"])

                # ä½¿ç”¨markdownæ˜¾ç¤ºå…¬å¼ï¼Œå¢å¼ºå¯è¯»æ€§
                st.markdown(
                    """
                <style>
                .formula-box {
                    background-color: #f8f9fa;
                    border-left: 4px solid #4e73df;
                    padding: 1rem;
                    margin: 1rem 0;
                    border-radius: 0.25rem;
                }
                .formula-step {
                    margin: 0.5rem 0;
                    padding: 0.5rem;
                    background-color: #ffffff;
                    border-radius: 0.25rem;
                }
                </style>
                """,
                    unsafe_allow_html=True,
                )

                # å°†å…¬å¼æ–‡æœ¬åˆ†å‰²æˆæ­¥éª¤æ˜¾ç¤º
                formula_lines = detail_info["å…¬å¼"]
                current_section = []

                for line in formula_lines:
                    if line.startswith("### "):
                        # æ˜¾ç¤ºä¹‹å‰çš„éƒ¨åˆ†
                        if current_section:
                            st.markdown(
                                f'<div class="formula-box">{"<br>".join(current_section)}</div>',
                                unsafe_allow_html=True,
                            )
                            current_section = []
                        # æ–°çš„å¤§æ ‡é¢˜
                        st.markdown(f"**{line[4:]}**")
                    elif line.startswith("#### "):
                        # æ˜¾ç¤ºä¹‹å‰çš„éƒ¨åˆ†
                        if current_section:
                            st.markdown(
                                f'<div class="formula-step">{"<br>".join(current_section)}</div>',
                                unsafe_allow_html=True,
                            )
                            current_section = []
                        # å°æ ‡é¢˜
                        st.markdown(f"*{line[5:]}*")
                    elif line.strip():
                        current_section.append(line)

                # æ˜¾ç¤ºæœ€åçš„éƒ¨åˆ†
                if current_section:
                    st.markdown(
                        f'<div class="formula-step">{"<br>".join(current_section)}</div>', unsafe_allow_html=True
                    )

                # æ˜¾ç¤ºLatexå…¬å¼æ€»ç»“
                st.write("### å…¬å¼æ€»ç»“")
                st.latex(
                    r"""
                \begin{aligned}
                \text{æ€»å‚æ•°} &= \text{Embedding} + N \times (\text{Attentionå±‚} + \text{MLPå±‚} + \text{LayerNormå±‚}) + \text{LM Head} \\
                \text{Embedding} &= V \times H \\
                \text{Attentionå±‚} &= 4 \times H^2 \\
                \text{MLPå±‚} &= 3 \times H \times I \quad (\text{Llamaæ¶æ„}) \\
                \text{LayerNormå±‚} &= 4 \times H \\
                \text{LM Head} &= H \times V
                \end{aligned}
                """
                )

                st.write("å…¶ä¸­ï¼š")
                st.write("- $V$ = è¯è¡¨å¤§å°")
                st.write("- $H$ = éšè—å±‚ç»´åº¦")
                st.write("- $I$ = ä¸­é—´å±‚ç»´åº¦")
                st.write("- $N$ = å±‚æ•°")

            else:
                st.warning("æœªèƒ½ç”Ÿæˆè¯¦ç»†è®¡ç®—å…¬å¼")

        with tab_details:
            st.subheader("ğŸ” è¯¦ç»†å‚æ•°ç»Ÿè®¡")

            if not df.empty:
                # å‚æ•°ç»Ÿè®¡æ‘˜è¦
                st.write("### å‚æ•°ç±»å‹ç»Ÿè®¡")
                type_stats = df.groupby("ParamType")["Count"].sum().reset_index()
                type_stats = type_stats.sort_values("Count", ascending=False)

                cols = st.columns(1)
                with cols[0]:
                    # æ˜¾ç¤ºç™¾åˆ†æ¯”
                    type_stats["Percentage"] = (type_stats["Count"] / type_stats["Count"].sum() * 100).round(2)
                    st.dataframe(type_stats[["ParamType", "Count", "Percentage"]], width="stretch")

                # å±‚å‚æ•°ç»Ÿè®¡
                st.write("### æ¯å±‚å‚æ•°ç»Ÿè®¡")
                layer_stats = df[df["LayerIdx"] >= 0].groupby("LayerIdx")["Count"].sum().reset_index()
                layer_stats = layer_stats.sort_values("LayerIdx")

                if not layer_stats.empty:
                    # è®¡ç®—å¹³å‡å€¼
                    avg_params_per_layer = layer_stats["Count"].mean()
                    st.info(f"å¹³å‡æ¯å±‚å‚æ•°: {format_number(int(avg_params_per_layer))}")

                    # æ˜¾ç¤ºå±‚å‚æ•°è¡¨æ ¼
                    st.dataframe(layer_stats, width="stretch")

                # è¯¦ç»†å‚æ•°è¡¨
                st.write("### å®Œæ•´å‚æ•°åˆ—è¡¨")
                st.dataframe(df[["Full Name", "Shape", "Count", "ParamType", "LayerIdx"]], width="stretch", height=500)
            else:
                st.warning("æœªèƒ½è§£æå‡ºè¯¦ç»†å‚æ•°ç»“æ„ã€‚")

        with tab_viz:
            st.subheader("ğŸ“ˆ å‚æ•°å¯è§†åŒ–")

            if not df.empty:
                # 1. å‚æ•°ç±»å‹åˆ†å¸ƒé¥¼å›¾
                col1, col2 = st.columns(2)

                with col1:
                    type_df = df.groupby("ParamType")["Count"].sum().reset_index()
                    fig1 = px.pie(type_df, values="Count", names="ParamType", title="å‚æ•°ç±»å‹åˆ†å¸ƒ", hole=0.3)
                    st.plotly_chart(fig1, width="stretch")

                with col2:
                    # 2. å±‚çº§åˆ†å¸ƒæ¡å½¢å›¾
                    if df["LayerIdx"].max() > 0:
                        layer_df = df[df["LayerIdx"] >= 0].groupby("LayerIdx")["Count"].sum().reset_index()
                        fig2 = px.bar(
                            layer_df,
                            x="LayerIdx",
                            y="Count",
                            title="å„å±‚å‚æ•°åˆ†å¸ƒ",
                            labels={"LayerIdx": "å±‚ç´¢å¼•", "Count": "å‚æ•°é‡"},
                        )
                        st.plotly_chart(fig2, width="stretch")

                # 3. Treemap
                st.write("### å±‚çº§ç»“æ„åˆ†å¸ƒå›¾")
                df_grouped = df.groupby(["Group", "LayerIdx", "SubGroup", "ParamType"])["Count"].sum().reset_index()
                df_grouped = df_grouped.sort_values(["Group", "LayerIdx"])

                fig3 = px.treemap(
                    df_grouped,
                    path=[px.Constant(model_input), "Group", "LayerIdx", "ParamType", "SubGroup"],
                    values="Count",
                    color="LayerIdx",
                    hover_data=["Count", "ParamType"],
                    title=f"{model_input} å‚æ•°å±‚çº§åˆ†å¸ƒ",
                )
                fig3.update_traces(textinfo="label+value")
                st.plotly_chart(fig3, width="stretch")
            else:
                st.warning("æ²¡æœ‰è¶³å¤Ÿçš„æ•°æ®è¿›è¡Œå¯è§†åŒ–ã€‚")

    else:
        status_container.update(label="âŒ å‡ºé”™äº†", state="error", expanded=True)
        st.error(f"æ— æ³•åŠ è½½æ¨¡å‹ä¿¡æ¯: {error_msg}")

        if "404" in error_msg or "not found" in error_msg.lower():
            st.warning("è¯·æ£€æŸ¥æ¨¡å‹ ID æ˜¯å¦æ‹¼å†™æ­£ç¡®ï¼Œæˆ–è€…è¯¥æ¨¡å‹æ˜¯å¦å­˜åœ¨ã€‚")
            st.markdown("ğŸ” ä½ å¯ä»¥åœ¨ [ModelScope](https://modelscope.cn/models) æœç´¢æ¨¡å‹")

        if "trust_remote_code" in error_msg:
            st.warning("ModelScopeæ¨¡å‹é€šå¸¸éœ€è¦Trust Remote Codeé€‰é¡¹ï¼Œè¯·ç¡®ä¿å·²å‹¾é€‰ã€‚")

        st.info("ğŸ’¡ **å¸¸è§é—®é¢˜è§£å†³æ–¹æ¡ˆ:**")
        st.markdown("1. ç¡®ä¿æ¨¡å‹IDæ ¼å¼æ­£ç¡®ï¼š`ç»„ç»‡å/æ¨¡å‹å`")
        st.markdown("2. å°è¯•å‹¾é€‰ **Trust Remote Code** é€‰é¡¹")

elif run_btn and not model_input:
    st.warning("è¯·è¾“å…¥ ModelScope æ¨¡å‹ IDã€‚")

# æ·»åŠ é¡µè„š
st.divider()
st.caption(
    """
**ğŸ’¡ ä½¿ç”¨æç¤º:**
- æœ¬å·¥å…·é€šè¿‡åˆ†ææ¨¡å‹é…ç½®å’Œç»“æ„è‡ªåŠ¨è®¡ç®—å‚æ•°é‡ï¼Œæ— éœ€ä¸‹è½½æƒé‡æ–‡ä»¶
- è¯¦ç»†å…¬å¼æ¨å¯¼åŸºäºTransformeræ¶æ„ï¼Œå¯¹äºéæ ‡å‡†æ¶æ„å¯èƒ½ç•¥æœ‰å·®å¼‚
- KV Cacheè®¡ç®—é€‚ç”¨äºDecoder-onlyè¯­è¨€æ¨¡å‹
"""
)

# æ·»åŠ ModelScopeé“¾æ¥
st.markdown("---")
st.markdown(
    "ğŸ”— [ModelScope å®˜ç½‘](https://modelscope.cn) | [ğŸ“š æ¨¡å‹åº“](https://modelscope.cn/models) | [ğŸ“– æ–‡æ¡£](https://modelscope.cn/docs)"
)
